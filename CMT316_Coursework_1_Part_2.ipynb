{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/W2HbiEbV74AYN9B2YIJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrLacia/CMT316-Coursework-1/blob/main/CMT316_Coursework_1_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpc7sARJnOLP",
        "outputId": "f7da5785-bfa7-46fa-cb94-7db258317b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import operator\n",
        "import requests\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to read the data"
      ],
      "metadata": {
        "id": "6Cz871itnRfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Tech Articles\n",
        "path = '/content/drive/My Drive/ML_Projects/datasets_coursework1/bbc/tech'\n",
        "files = os.listdir(path)\n",
        "techArticles = []\n",
        "for file in files:\n",
        "  techArticles.append(open(path + '/' + file).read())\n",
        "\n",
        "# Sport Articles\n",
        "path = '/content/drive/My Drive/ML_Projects/datasets_coursework1/bbc/sport'\n",
        "files = os.listdir(path)\n",
        "sportArticles = []\n",
        "for file in files:\n",
        "  sportArticles.append(open(path + '/' + file).read())\n",
        "\n",
        "# Politics Articles\n",
        "path = '/content/drive/My Drive/ML_Projects/datasets_coursework1/bbc/politics'\n",
        "files = os.listdir(path)\n",
        "politicsArticles = []\n",
        "for file in files:\n",
        "  politicsArticles.append(open(path + '/' + file).read())\n",
        "\n",
        "# Entertainment Articles\n",
        "path = '/content/drive/My Drive/ML_Projects/datasets_coursework1/bbc/entertainment'\n",
        "files = os.listdir(path)\n",
        "entertainmentArticles = []\n",
        "for file in files:\n",
        "  entertainmentArticles.append(open(path + '/' + file).read())\n",
        "\n",
        "# Business Articles\n",
        "path = '/content/drive/My Drive/ML_Projects/datasets_coursework1/bbc/business'\n",
        "files = os.listdir(path)\n",
        "businessArticles = []\n",
        "for file in files:\n",
        "  businessArticles.append(open(path + '/' + file).read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riRIIItJnVpP",
        "outputId": "b5f0126a-e269-4e23-80c2-9c8b446348d5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the data needs to go through pre-processing"
      ],
      "metadata": {
        "id": "N3adunJBQX9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# First, we get the stopwords list from nltk\n",
        "stopwords=nltk.corpus.stopwords.words('english')\n",
        "# We can add more words to the stopword list, like punctuation marks\n",
        "extraStopwords = [\".\",\",\",\"''\",\"-\",\"'\",\"'s\",\"n't\",\"``\",\"%\",\"(\",\")\",\"said\",\"also\",\"get\",\"says\",\"many\",\"mr\",\"mrs\",\"miss\",\":\",\";\",\"'m\"]\n",
        "stopwords.extend(extraStopwords)\n",
        "\n",
        "def getPrelimTokens(articles):\n",
        "  tempTokens = []\n",
        "  for article in articles:\n",
        "    tempTokens.append(nltk.tokenize.word_tokenize(article))\n",
        "\n",
        "  for article in tempTokens:\n",
        "    lemma = []\n",
        "    for token in article:\n",
        "      lemma.append(lemmatizer.lemmatize(token))\n",
        "    article = lemma\n",
        "  tempTokens = [[token.lower() for token in article] for article in tempTokens]\n",
        "\n",
        "  tempFreq={}\n",
        "  for article in tempTokens:\n",
        "    for token in article:\n",
        "      if token in stopwords:\n",
        "        continue\n",
        "      elif token in tempFreq:\n",
        "        tempFreq[token] += 1\n",
        "      else:\n",
        "        tempFreq[token] = 1\n",
        "\n",
        "  return sorted(tempFreq.items(), key=operator.itemgetter(1), reverse=True)[:1000]\n",
        "\n",
        "# Tech\n",
        "sortedTechFreq = getPrelimTokens(techArticles)\n",
        "\n",
        "# Sport Articles\n",
        "sortedSportFreq = getPrelimTokens(sportArticles)\n",
        "\n",
        "# Politics Articles\n",
        "sortedPoliticsFreq = getPrelimTokens(politicsArticles)\n",
        "\n",
        "# Entertainment Articles\n",
        "sortedEntertainmentFreq = getPrelimTokens(entertainmentArticles)\n",
        "\n",
        "# Business Articles\n",
        "sortedBusinessFreq = getPrelimTokens(businessArticles)\n",
        "\n",
        "# Overall\n",
        "overallFrequencies = {}\n",
        "tempList = sortedTechFreq + sortedBusinessFreq + sortedSportFreq + sortedEntertainmentFreq + sortedPoliticsFreq\n",
        "\n",
        "for word,frequency in tempList:\n",
        "  if word in overallFrequencies:\n",
        "    overallFrequencies[word] += int(frequency)\n",
        "  else:\n",
        "    overallFrequencies[word] = int(frequency)\n",
        "\n",
        "sortedOverallFreq = sorted(overallFrequencies.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "# Assign vocabulary using the overal frequency list\n",
        "vocab=[]\n",
        "for word,frequency in sortedOverallFreq:\n",
        "  vocab.append(word)\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2To4WBWaQa_Z",
        "outputId": "a66ef9f0-fc91-4263-f47b-9bf4854086d5"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the vocabulary that has been formed by combining the 1000 most common tokens from each of the five catagories we can now conver the articles to vectors\n",
        "\n",
        "We are using three different features:\n",
        "\n",
        "\n",
        "1. Absolute Frequency of the words present in the defined vocabulary\n",
        "2. Average Sentence Length\n",
        "3. Proportion of words inside quotes\n",
        "\n"
      ],
      "metadata": {
        "id": "LhZYRcDw67by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function adapted from the exercise '1_Introduction_DataPreprocessing_NLTK_Numpy'\n",
        "def get_list_tokens(string):\n",
        "  tokens = nltk.tokenize.word_tokenize(string)\n",
        "  for token in tokens:\n",
        "    token = lemmatizer.lemmatize(token)\n",
        "  tokens = [token.lower() for token in tokens]\n",
        "  return tokens\n",
        "\n",
        "# Function adapted from the exercise '2_FeatureEngineeringSelection_Sklearn'\n",
        "def get_vector_text(list_vocab,string):\n",
        "  # Absolute frequency\n",
        "  vector_text=np.zeros(len(list_vocab)+2)\n",
        "  list_tokens_string=get_list_tokens(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "\n",
        "  # Average Sentence Length\n",
        "  avgSenLen = len(list_tokens_string) / len(nltk.tokenize.sent_tokenize(string))\n",
        "  vector_text[-2] = avgSenLen\n",
        "\n",
        "  # Relative Number of Words inside Quotes\n",
        "  quotes = re.findall(\"\\\"(.*?)\\\"|\\'(.*?)\\'\", string)\n",
        "  wordsInQuotes = 0\n",
        "  if not(quotes is None):\n",
        "    for quote in quotes:\n",
        "      wordsInQuotes += len(nltk.tokenize.word_tokenize(quote[0]))\n",
        "  vector_text[-1] = wordsInQuotes / len(list_tokens_string)\n",
        "\n",
        "  return vector_text"
      ],
      "metadata": {
        "id": "MQxvoiN26wgL"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After converting the articles to vectors they are going to be kept seperate so that test data can be extracted evenly across the five classifications\n",
        "In addition files will be labeled as follows:\n",
        "\n",
        "\n",
        "1. Tech\n",
        "2. Sport\n",
        "3. Politics\n",
        "4. Entertainment\n",
        "5. Business\n",
        "\n"
      ],
      "metadata": {
        "id": "1XnosVH28sx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xTrain = []\n",
        "xTest = []\n",
        "xDev= []\n",
        "yTrain = []\n",
        "yTest = []\n",
        "yDev = []\n",
        "\n",
        "def processData(articles, label):\n",
        "  xTemp = []\n",
        "  for article in articles:\n",
        "    vector = get_vector_text(vocab,article)\n",
        "    xTemp.append(vector)\n",
        "\n",
        "  # Shuffle and remove 10% for the test data, and 10% for the development data\n",
        "  random.shuffle(xTemp)\n",
        "  xTest.extend(xTemp[:len(xTemp)//10])\n",
        "  yTest.extend([label] * len(xTemp[:len(xTemp)//10]))\n",
        "  xDev.extend(xTemp[len(xTemp)//10:len(xTemp)//5])\n",
        "  yDev.extend([label] * len(xTemp[len(xTemp)//10:len(xTemp)//5]))\n",
        "  xTrain.extend(xTemp[len(xTemp)//5:])\n",
        "  yTrain.extend([label] * len(xTemp[len(xTemp)//5:]))\n",
        "\n",
        "# Tech\n",
        "processData(techArticles, 1)\n",
        "\n",
        "# Sport\n",
        "processData(sportArticles, 2)\n",
        "\n",
        "# Politics\n",
        "processData(politicsArticles, 3)\n",
        "\n",
        "# Entertainment\n",
        "processData(entertainmentArticles, 4)\n",
        "\n",
        "# Business\n",
        "processData(businessArticles, 5)\n",
        "\n",
        "print(len(xTest))\n",
        "print(len(xDev))\n",
        "print(len(xTrain))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVp6yNja7GZi",
        "outputId": "dcfd37c3-e43c-4598-95bd-7d27c1c3e4ea"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221\n",
            "223\n",
            "1780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection and Evaluation of Number of Features and Regularization parameter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ko_UZfNwFcRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xTrainArray=np.asarray(xTrain)\n",
        "yTrainArray=np.asarray(yTrain)\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluation of Number of Features and Regularization parameter\n",
        "bestAcc = 0.0\n",
        "bestParam = 0\n",
        "bestNumFeatures = 0\n",
        "for num in range (1,5):\n",
        "  featureSelection=SelectKBest(chi2, k=int(num*250)).fit(xTrainArray, yTrainArray)\n",
        "  xTrainNew = featureSelection.transform(xTrain)\n",
        "  print(\"Number of Features: \" + str(int(num*250)))\n",
        "  for param in range(1,21):\n",
        "    svmClass=sklearn.svm.SVC(kernel=\"rbf\",gamma='scale',C=(param/10)) # Initialize the SVM model\n",
        "    svmClass.fit(xTrainNew,yTrainArray) # Train the SVM model\n",
        "    tempPredictions = svmClass.predict(featureSelection.transform(xDev))\n",
        "    print(str(param/10) + \"\\t:\\t\" + str(accuracy_score(yDev, tempPredictions)))\n",
        "    if accuracy_score(yDev, tempPredictions) > bestAcc:\n",
        "      bestNumFeatures = int(num*250)\n",
        "      bestAcc = accuracy_score(yDev, tempPredictions)\n",
        "      bestParam = param/10\n",
        "\n",
        "print(\"Best number of features: \" + str(bestNumFeatures))\n",
        "print(\"Best Value for C Regularization Parameter: \" + str(bestParam))\n",
        "print(\"Best Accuracy Score: \" + str(bestAcc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Q7QgtAFqLK",
        "outputId": "d4bad436-2a3c-4086-f3ca-7e15c1d74566"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Features: 250\n",
            "0.1\t:\t0.8475336322869955\n",
            "0.2\t:\t0.9013452914798207\n",
            "0.3\t:\t0.9192825112107623\n",
            "0.4\t:\t0.9282511210762332\n",
            "0.5\t:\t0.9327354260089686\n",
            "0.6\t:\t0.9417040358744395\n",
            "0.7\t:\t0.9506726457399103\n",
            "0.8\t:\t0.9506726457399103\n",
            "0.9\t:\t0.9506726457399103\n",
            "1.0\t:\t0.9506726457399103\n",
            "1.1\t:\t0.9506726457399103\n",
            "1.2\t:\t0.9506726457399103\n",
            "1.3\t:\t0.9551569506726457\n",
            "1.4\t:\t0.9551569506726457\n",
            "1.5\t:\t0.9551569506726457\n",
            "1.6\t:\t0.9551569506726457\n",
            "1.7\t:\t0.9551569506726457\n",
            "1.8\t:\t0.9551569506726457\n",
            "1.9\t:\t0.9596412556053812\n",
            "2.0\t:\t0.9596412556053812\n",
            "Number of Features: 500\n",
            "0.1\t:\t0.8565022421524664\n",
            "0.2\t:\t0.9192825112107623\n",
            "0.3\t:\t0.9327354260089686\n",
            "0.4\t:\t0.9551569506726457\n",
            "0.5\t:\t0.9551569506726457\n",
            "0.6\t:\t0.9506726457399103\n",
            "0.7\t:\t0.9506726457399103\n",
            "0.8\t:\t0.9551569506726457\n",
            "0.9\t:\t0.9551569506726457\n",
            "1.0\t:\t0.9551569506726457\n",
            "1.1\t:\t0.9551569506726457\n",
            "1.2\t:\t0.9596412556053812\n",
            "1.3\t:\t0.9596412556053812\n",
            "1.4\t:\t0.9596412556053812\n",
            "1.5\t:\t0.9596412556053812\n",
            "1.6\t:\t0.9551569506726457\n",
            "1.7\t:\t0.9551569506726457\n",
            "1.8\t:\t0.9551569506726457\n",
            "1.9\t:\t0.9551569506726457\n",
            "2.0\t:\t0.9596412556053812\n",
            "Number of Features: 750\n",
            "0.1\t:\t0.8654708520179372\n",
            "0.2\t:\t0.9103139013452914\n",
            "0.3\t:\t0.9327354260089686\n",
            "0.4\t:\t0.9506726457399103\n",
            "0.5\t:\t0.9596412556053812\n",
            "0.6\t:\t0.9551569506726457\n",
            "0.7\t:\t0.9596412556053812\n",
            "0.8\t:\t0.9596412556053812\n",
            "0.9\t:\t0.9551569506726457\n",
            "1.0\t:\t0.9551569506726457\n",
            "1.1\t:\t0.9551569506726457\n",
            "1.2\t:\t0.9596412556053812\n",
            "1.3\t:\t0.9596412556053812\n",
            "1.4\t:\t0.9596412556053812\n",
            "1.5\t:\t0.9596412556053812\n",
            "1.6\t:\t0.9596412556053812\n",
            "1.7\t:\t0.9596412556053812\n",
            "1.8\t:\t0.9596412556053812\n",
            "1.9\t:\t0.9551569506726457\n",
            "2.0\t:\t0.9551569506726457\n",
            "Number of Features: 1000\n",
            "0.1\t:\t0.8430493273542601\n",
            "0.2\t:\t0.9103139013452914\n",
            "0.3\t:\t0.9417040358744395\n",
            "0.4\t:\t0.9506726457399103\n",
            "0.5\t:\t0.968609865470852\n",
            "0.6\t:\t0.9641255605381166\n",
            "0.7\t:\t0.9641255605381166\n",
            "0.8\t:\t0.9641255605381166\n",
            "0.9\t:\t0.968609865470852\n",
            "1.0\t:\t0.9641255605381166\n",
            "1.1\t:\t0.9641255605381166\n",
            "1.2\t:\t0.9641255605381166\n",
            "1.3\t:\t0.9641255605381166\n",
            "1.4\t:\t0.9641255605381166\n",
            "1.5\t:\t0.9641255605381166\n",
            "1.6\t:\t0.9641255605381166\n",
            "1.7\t:\t0.9641255605381166\n",
            "1.8\t:\t0.9641255605381166\n",
            "1.9\t:\t0.9641255605381166\n",
            "2.0\t:\t0.9641255605381166\n",
            "Best number of features: 1000\n",
            "Best Value for C Regularization Parameter: 0.5\n",
            "Best Accuracy Score: 0.968609865470852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Final Model"
      ],
      "metadata": {
        "id": "dznkJnjkFvK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment these lines to use the parameter outlined in the report\n",
        "bestNumFeatures = 1000\n",
        "bestParam = 1.3\n",
        "\n",
        "# Train model using optimal parameters\n",
        "featureSelection=SelectKBest(chi2, k=bestNumFeatures).fit(xTrainArray, yTrainArray)\n",
        "xTrainNew = featureSelection.transform(xTrain)\n",
        "svmClass=sklearn.svm.SVC(kernel=\"rbf\",gamma='scale',C=bestParam) # Initialize the SVM model\n",
        "svmClass.fit(xTrainNew,yTrainArray) # Train the SVM model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "4sJTt1DtFeXt",
        "outputId": "89377bbd-db64-4afa-feed-66bc07185466"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.3)"
            ],
            "text/html": [
              "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1.3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1.3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Results"
      ],
      "metadata": {
        "id": "JIGtDvQlFxFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Classification\n",
        "yPredictions = svmClass.predict(featureSelection.transform(xTest))\n",
        "\n",
        "print(accuracy_score(yTest, yPredictions))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\n\", classification_report(yTest, yPredictions))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(yTest, yPredictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0y4MdBTJ_6n",
        "outputId": "5a9826ab-1dc2-44eb-e04d-2a574fc33fe5"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9683257918552036\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      1.00      0.96        40\n",
            "           2       1.00      1.00      1.00        51\n",
            "           3       0.90      0.93      0.92        41\n",
            "           4       1.00      1.00      1.00        38\n",
            "           5       1.00      0.92      0.96        51\n",
            "\n",
            "    accuracy                           0.97       221\n",
            "   macro avg       0.97      0.97      0.97       221\n",
            "weighted avg       0.97      0.97      0.97       221\n",
            "\n",
            "[[40  0  0  0  0]\n",
            " [ 0 51  0  0  0]\n",
            " [ 3  0 38  0  0]\n",
            " [ 0  0  0 38  0]\n",
            " [ 0  0  4  0 47]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ehYBAJl9wU7"
      }
    }
  ]
}